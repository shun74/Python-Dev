{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.vision_v1.types.image_annotator.EntityAnnotation"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile scan_text.py\n",
    "import io\n",
    "import os\n",
    "from google.cloud import vision\n",
    "\n",
    "cred_path = r'C:\\Users\\syunn\\Documents\\Dev\\Scanner\\my_cred.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cred_path\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "file_name = os.path.abspath('Image/dice.jpg')\n",
    "\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision.Image(content=content)\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "type(response.text_annotations[0])\n",
    "# for text in response.text_annotations:\n",
    "#     print(text.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "coordinate list must contain exactly 2 coordinates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5ebd7bbf28ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Image/dice.jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Image/dice_flame.jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mhighlight_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mread_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-5ebd7bbf28ca>\u001b[0m in \u001b[0;36mhighlight_faces\u001b[1;34m(image, faces, output_filename)\u001b[0m\n\u001b[0;32m     28\u001b[0m         box = [(vertex.x, vertex.y)\n\u001b[0;32m     29\u001b[0m                for vertex in face.bounding_poly.vertices]\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Place the confidence value/score of the detected faces above the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# detection box in the output image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mrectangle\u001b[1;34m(self, xy, fill, outline, width)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfill\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_rectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mink\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mink\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfill\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_rectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: coordinate list must contain exactly 2 coordinates"
     ]
    }
   ],
   "source": [
    "# %%writefile add_frame.py\n",
    "import io\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from google.cloud import vision\n",
    "\n",
    "def read_img(image_file):\n",
    "    # Instantiates a client\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # The name of the image file to annotate\n",
    "    file_name = os.path.abspath(image_file)\n",
    "\n",
    "    # Loads the image into memory\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "\n",
    "    return response.text_annotations\n",
    "\n",
    "def highlight_faces(image, faces, output_filename):\n",
    "    im = Image.open(image)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    # Sepecify the font-family and the font-size\n",
    "    for face in faces:\n",
    "        box = [(vertex.x, vertex.y)\n",
    "               for vertex in face.bounding_poly.vertices]\n",
    "        draw.rectangle(box + [box[0]], fill=(0,255,0))\n",
    "        # Place the confidence value/score of the detected faces above the\n",
    "        # detection box in the output image\n",
    "    im.save(output_filename)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    cred_path = r'C:\\Users\\syunn\\Documents\\Dev\\Scanner\\my_cred.json'\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cred_path\n",
    "    image = \"Image/dice.jpg\"\n",
    "    output = \"Image/dice_flame.jpg\"\n",
    "    highlight_faces(image,read_img(image),output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crop Hint: 0\n",
      "bounds: (144,0),(685,0),(685,540),(144,540)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "from google.cloud import vision\n",
    "\n",
    "def detect_crop_hints(path,output_filename):\n",
    "    \"\"\"Detects crop hints in an image.\"\"\"\n",
    "    \n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    crop_hints_params = vision.CropHintsParams(aspect_ratios=[1])\n",
    "    image_context = vision.ImageContext(crop_hints_params=crop_hints_params)\n",
    "\n",
    "    response = client.crop_hints(image=image, image_context=image_context)\n",
    "    hints = response.crop_hints_annotation.crop_hints\n",
    "\n",
    "    for n, hint in enumerate(hints):\n",
    "        print('\\nCrop Hint: {}'.format(n))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y) for vertex in hint.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))\n",
    "    \n",
    "    im = Image.open(path)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    for hint in hints:\n",
    "        box = [(vertex.x, vertex.y) for vertex in hint.bounding_poly.vertices]\n",
    "        draw.line(box + [box[0]], width=2, fill='#00ff00')\n",
    "        # Place the confidence value/score of the detected faces above the\n",
    "        # detection box in the output image\n",
    "    im.save(output_filename)\n",
    "    \n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    cred_path = r'C:\\Users\\syunn\\Documents\\Dev\\Scanner\\my_cred.json'\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cred_path\n",
    "    image = \"Image/sample1.jpg\"\n",
    "    output = \"Image/sample1_croped.jpg\"\n",
    "    detect_crop_hints(image,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scan.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scan.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "from google.cloud import vision\n",
    "\n",
    "class Scan():\n",
    "    image_path = \"\"\n",
    "    output_path = \"\"\n",
    "    dictionary = []\n",
    "\n",
    "    def __init__(self,image_path=None,output_path=None,dictionary=[]):\n",
    "        self.image_path = image_path\n",
    "        self.output_path = output_path\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def get_annotations(self):\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        with io.open(self.image_path, 'rb') as image:\n",
    "            content = image.read()\n",
    "        vision_image = vision.Image(content=content)\n",
    "        response = client.text_detection(image=vision_image)\n",
    "        return response.text_annotations\n",
    "    \n",
    "    def search_dictionary(self,annotations=None):\n",
    "        descriptions = [data.description for data in annotations]\n",
    "        flags = [False] * len(descriptions)\n",
    "        for i,text in enumerate(descriptions):\n",
    "            for j in range(len(descriptions) - i - 1):\n",
    "                for word in self.dictionary:\n",
    "                    if text == word:\n",
    "                        for k in range(j+1):\n",
    "                            flags[i+k] = True\n",
    "                text += descriptions[i+j+1]\n",
    "        return flags\n",
    "    \n",
    "    def redraw_image(self,annotations=None,flags=[]):\n",
    "        vertices = [data.bounding_poly for data in annotations]\n",
    "        im = Image.open(self.image_path)\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        for i, flag in enumerate(flags):\n",
    "            if flag == True:\n",
    "                box = [(vertices[i].vertices[2].x, vertices[i].vertices[2].y), (vertices[i].vertices[3].x, vertices[i].vertices[3].y)]\n",
    "                draw.line((box[0], box[1]), width=2, fill='#00ff00')\n",
    "        try:\n",
    "            im.save(self.output_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    cred_path = r'C:\\Users\\syunn\\Documents\\Dev\\Scanner\\my_cred.json'\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cred_path\n",
    "\n",
    "    scan = Scan(image_path=\"Image/sample1.jpg\",output_path=\"Image/sample1_scan.jpg\",dictionary=[\"酸\",\"この表示\",\"安藤\"])\n",
    "    annotations = scan.get_annotations()\n",
    "    flags = scan.search_dictionary(annotations)\n",
    "    if scan.redraw_image(annotations,flags):\n",
    "        print(\"scan is completed.\")\n",
    "    else:\n",
    "        print(\"scan is failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
